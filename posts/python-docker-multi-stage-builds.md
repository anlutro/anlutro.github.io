# Multi-stage Docker builds for Python projects
pubdate: 2018-08-06 21:34 CEST
tags: Python, Docker

Multi-stage builds can help reduce your Docker image sizes in production. This has many benefits: Development dependencies may potentially expose extra security holes in your system (I've yet to see this happen, but why not be cautious if it's easy to be so?), but mostly by reducing image size you make it faster for others to `docker pull` it.

The concept of multi-stage builds is simple: Install development dependencies, build all the stuff you need, then copy over just the stuff you need to run in production in a brand new image without installing development dependencies not needed to run the application.

Here's an example Dockerfile using the official Python Docker images, which are based on Debian - but you can easily apply the same principle when building from Debian, Ubuntu, CentOS, or Alpine images: Have one part of your Dockerfile be the stage where the application is built and dependencies are installed, another where the application is actually ran.

	FROM python:3.7-stretch AS build
	RUN python3 -m venv /venv

	# example of a development library package that needs to be installed
	RUN apt-get update && apt-get install libldap2-dev && \
	    rm -rf /var/cache/apt/* /var/lib/apt/lists/*

	# install requirements separately to prevent pip from downloading and
	# installing pypi dependencies every time a file in your project changes
	ADD ./requirements /project/requirements
	RUN /venv/bin/pip install -r /project/requirements/base.txt

	# install the project, basically copying its code, into the virtualenv.
	# this assumes the project has a functional setup.py
	ADD . /project
	RUN /venv/bin/pip install /project


	# the second, production stage can be much more lightweight:
	FROM python:3.7-stretch-slim AS production
	COPY --from=build /venv /venv

	# install runtime libraries (different from development libraries!)
	RUN apt-get update && apt-get install libldap-2.4-2 && \
	    rm -rf /var/cache/apt/* /var/lib/apt/lists/*

	# remember to run python from the virtualenv
	CMD ["/venv/bin/python3", "-m", "myproject"]

Copying the virtual environment is by far the easiest approach to this problem. Python purists will say that virtual environments shouldn't be copied, but when the underlying system is the same and the path is the same, it makes literally no difference (plus virtualenvs are a dirty hack to begin with, one more dirty hack doesn't make a difference). There are alternate approaches such as downloading pypi packages or building dependencies as wheels and then copy those over, but it's more complicated and doesn't really have any benefits.

In our example, we install both project dependencies *and* the project itself into the virtualenv. This means we don't even need the project root directory in the production image. This does mean that the docker image isn't suited for development. If you want that, you'll have to change `pip install .` to `pip install -e .`, and you'll have to add the `/project` directory to the production image as well.

You can also build a third stage which includes dev/test dependencies, which might be useful for continuous integration systems. We'll base this stage on the image generated by the previous build stage:

	FROM build AS development
	RUN /venv/bin/pip install -r /project/requirements/dev.txt
	WORKDIR /project
	CMD ["/venv/bin/pytest"]

You should put this stage *in the middle* of the two other stages of the Dockerfile, in order to preserve the production image as the default image to be built when you run `docker build`. To build and run this image in particular, use the `--target` argument:

	$ docker build --target=development --tag=myproject-dev .
	$ docker run --rm -it myproject-dev
